T-VLAD: Temporal vector of locally aggregated descriptor for multiview human :pouting_man: action recognition
-
Complete code of the research article :page_with_curl: that can be cited as:

"Naeem, H. B., Murtaza, F., Yousaf, M. H., & Velastin, S. A. (2021). T-VLAD: Temporal vector of locally aggregated descriptor for multiview human action recognition. Pattern Recognition Letters, 148, 22-28."

This article studies view-invariant human action recognition. Major contributions are:

- Explores view-invariant action representation in multi-view videos using convolutional features.
- Current methods are computationally expensive and limit temporal coverage in short segments.
- Proposed framework uses a temporal vector of locally aggregated descriptors (T-VLAD) to encode long-term video temporal structure.
- T-VLAD encodes video's long-term temporal structure using single stream convolutional features over short segments.
- Results validated in a challenging setup with one view for testing and remaining views for training.
- State-of-the-art results obtained on three multi-view datasets with fixed cameras and a dynamic background dataset, UCF101.

![Figure1](https://github.com/user-attachments/assets/c6a22dcc-1555-4ba0-8a28-498fc2a1e205)

__Keywords:__ Human action recognition; Multi-view; View-invariant; Temporal action sequence; VLAD; 3D Convolutional neural network features; IXMAS; Muhavi; UCF101; Short segment features

__Download article from the link below:__ 
https://drive.google.com/file/d/1Ymm32c41BgePnPBX4JWXCtFCiL1k3Tpc/view?usp=sharing

Having issue running code? 
__Contact me at:__ :email: hajra_naeem19@yahoo.com



  
